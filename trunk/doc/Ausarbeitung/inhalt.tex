% $Log$
% Revision 1.6  2006/01/09 13:26:55  kelsaka
% - beschreibung des komponentenmodells ergänzt
%
% Revision 1.5  2006/01/09 09:49:10  kelsaka
% - beschreibung des komponentenmodells weiter ergänzt
%
% Revision 1.4  2006/01/08 17:43:03  kelsaka
% - beschreibung des komponentenmodells weiter ergänzt
%
% Revision 1.3  2006/01/08 15:42:41  kelsaka
% - beschreibung des komponentenmodells ergänzt
%
% Revision 1.1  2006/01/05 16:46:14  kelsaka
% - initial creation
%


% TODO: auf rechte Seite prüfen --> je nach TOC
\section{Abstract}
\label{sec:Abstract}
\begin{abstract}
	TODO: Abstract
\end{abstract}

\newpage
%	leere Seite
$\,$
\newpage



\section{Einleitung}
\label{sec:Einleitung}



Ist im Folgenden von \textit{Komponentenmodell} die Rede, so ist, sofern nicht anders angegeben, das \textit{Palladio Komponentenmodell} gemeint. Als Palladio Komponentenmodell wird dabei das \textit{Meta}-Modell zur Darstellung von Komponentenarchitekturen der Palladio-Gruppe \cite{PALL} bezeichnet. Dieses Modell wird in Kapitel \ref{sec:DasPalladioKomponentenmodell} näher beschrieben.

Zudem bezeichnet der Begriff \textit{Projekt}, solange er ohne sonstigen Kontext verwendet wird, die Diplomarbeit.



\section{Das Palladio Komponentenmodell}
\label{sec:DasPalladioKomponentenmodell}
Im Rahmen der Diplomarbeit wird ein Meta-Modell für das Palladio Komponentenmodell erstellt. Daher ist es unabdingbar, dass ein vollständiges Verständnis für das Komponentenmodell existiert. Die folgenden Kapitel zeigen detailliert die Eigenschaften des Komponentenmodells auf. Schließlich wird dem gegenübergestellt, welche Konzepte des Komponentenmodells umgesetzt werden konnten, welche Einschränkungen bei der Modellierung auftraten und welche \textit{Work"=Arounds} aus welchen Gründen zur Abbildung notwendig waren.



\subsection{Einfache Komponenten}
\label{sec:EinfacheKomponenten}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.70\textwidth]{./image/cm-simple-component-01.pdf}
	\caption{Einfache Komponente mit einer angebotener und zwei be\-nö\-tig\-ten Schnittstellen}
	\label{fig:cm-simple-component-01}
\end{figure}



Das Palladio Komponentenmodell -- unter \cite{BECK} findet sich die Beschreibung einer älteren Fassung des Modells -- beschreibt Software-Architekturen als eine Menge von Komponenten und Schnittstellen, sowie darauf definierten Relationen. In Abbildung \ref{fig:cm-simple-component-01} wird eine einfache Komponente "`Component"' gezeigt, die eine Schnittstelle anbietet (\textit{Provided Interface}), im Beispiel "`Provided Interface 1"' genannt. Daneben benötigt die Komponente zwei Schnittstellen ("`Required Interface 1 / 2"').

Im Folgenden sollen zunächst einfache Basiskonstrukte des Komponentenmodell dargestellt werden. Eine detaillierte Erklärung, welche Konstrukte in welchen Situationen als valide bewertet werden, folgt in den nächsten Kapiteln (ref TODO: Kapitelnummer). In den Abbildungen dieses Kapitel wird auf die standardisierte Visualisierung mit Hilfe von UML 2 Komponentendiagrammen \cite{uml2} zurückgegriffen.

Eine Komponente kann \texttt{0..*} Schnittstellen anbieten. Das bedeutet, dass die Komponenten die Dienste auch anbieten muss, die über eine Schnittstelle festgelegt sind. Der Aufbau einer Schnittstelle wird in Kapitel ref{TODO} beschrieben. Für eine Komponente darf im Allgemeinen jedoch nicht angenommen werden, das angebotene Schnittstellen und darauf definierte Dienste auch tatsächlich angesprochen werden.

Auf der \textit{Requires}-Seite ist eine Komponente ebenso frei und kann \texttt{0..*} Schnittstellen benötigen. Durch die \textit{Requires}-Schnittstelle definiert eine Komponente die Dienste, die sie selbst benötigt. Wird eine Komponente verwendet, muss für eine vollständige uneingeschränkte Verwendung sichergestellt werden, dass die benötigten Dienste ebenso vollständig von anderen Komponenten angeboten werden und erreicht werden können (zur Verbindung zwischen Komponenten siehe Kapitel ref{TODO}).

Eine Komponente kann im Übrigen die gleiche Schnittstelle anbieten und zugleich verlangen. Stellt man sich beispielsweise eine \textit{Chain-of-Responsibility} (vgl. \cite{GAMMA}) vor, die über Komponenten realisiert wird, so wird die gleiche Schnittstelle zur Annahme (\textit{provided}) von Dienstaufrufen und zur Weitergabe von Dienstaufrufen an den Nachfolger in der Kette verwendet.



\subsection{Schnittstellen und Rollen}
\label{sec:SchnittstellenUndRollen}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.55\textwidth]{./image/cm-interfaces-roles-01.pdf}
	\caption{Angebotene und benötigte Schnittstellen mit ihren dazugehörigen Rollen in UML2"=Notation}
	\label{cm-interfaces-roles-01}
\end{figure}



Über Abbildung \ref{cm-interfaces-roles-01} soll verdeutlicht werden, welche Bestandteile der graphischen Notation, die auch im Folgenden verwendet wird, einem Interface entsprechen. Auf der linken Seite ist eine Komponente angedeutet, die eine Schnittstelle anbietet. Bietet eine Komponente eine Schnittstelle an (\textit{Provided Interface}), so impliziert dies, dass eine \textit{Provides Role} zwischen Komponente und Interface existiert. In der graphischen Notation ist die vergleichbar mit der Verbindungslinie zwischen Schnittstelle und Komponente. Wird eine Komponente angeboten, resultiert dies in einer Kreisdarstellung für die Schnittstelle.

Auf der rechten Seite der Abbildung ist die \textit{Requires Role} zwischen einer Komponente unter der Schnittstelle dargestellt. Auch hier entspricht die Verbindungslinie zwischen Komponente und Schnittstelle der Rollen-Beziehung. Die graphische Notation ändert sich jedoch. Aus dem Kreis, der die Schnittstelle symbolisiert, wird ein Halbkreis.

Auch im Komponentenmodell wird die Relation zwischen Komponente und Schnittstelle als "`Rolle"' (\textit{Role}) bezeichnet. Die Art der Rolle (angeboten oder benötigt / \textit{provided} oder \textit{required}) entscheidet über die Verwendung der Schnittstelle. Die Eigenschaft der Schnittstelle selbst ändert sich jedoch nicht durch ihre Verwendung. Auch in Abbildung \ref{cm-interfaces-roles-01} könnten die Komponenten die beiden dargestellten Schnittstellen identisch sein.

Eine Schnittstelle wird also erst durch eine \textit{Provides Role} zu einer angebotene Schnittstelle / einem \textit{Provided Interface}, bzw. über eine \textit{Requires Role} zu einer benötigten Schnittstelle / einem\textit{Required Interface}.



\subsection{Assembly Konnektoren}
\label{sec:Assembly Konnektoren}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/cm-assembly-connector-01.pdf}
	\caption{Eine Komponente wird über Assembly Konnektoren mit zwei weiteren Komponenten verbunden}
	\label{fig:cm-assembly-connector-01}
\end{figure}



In Abbildung \ref{fig:cm-assembly-connector-01} wird die Verwendung zweier Assembly Konnektoren (\textit{Assembly Connector}) dargestellt. Präzise betrachtet verbinden Konnektoren je eine angebotene und eine benötigte Rolle einer Schnittstelle. Dies bedeutet, dass Aufrufe der über die \textit{Requires Role} mit der benötigten Schnittstelle verbundenen Komponente auf die angebotene Schnittstelle und damit auf die über eine \textit{Provides Role} damit verbundene Komponente weitergeleitet werden. Der Kontrollfluß folgt damit in diesem Fall der als gestrichelt dargestellten Linie.

Jede benötigte Schnittstelle einer Komponente wird damit mit genau einer angebotenen Schnittstelle verbunden. Sind die angebotene Schnittstelle und die benötigte Schnittstelle einer Komponente identisch oder kompatibel (Def. siehe TODO), so kann eine Komponente grundsätzlich auch eigene Dienste aufrufen. In der noch folgenden Differenzierung über die Typ-Ebenen des Komponentenmodells (siehe Kapitel ref{TODO}) wird deutlich, weshalb dies nicht zwangsläufig zu einer infiniten Rekursion führen muss.



\subsection{Zusammengesetzte Komponenten}
\label{sec:ZusammengesetzteKomponenten}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/cm-composite-component-01.pdf}
	\caption{Eine zusammengesetzte Komponente mit inneren Komponenten}
	\label{fig:cm-composite-component-01}
\end{figure}



Zusammengesetzte Komponenten, im Folgenden auch \textit{Composite Components} genannt, enthalten eine interne Realisierung über weitere Komponenten und über zugehörige Strukturen. Ein \textit{Composite Component} selbst enthält keine Realisierung etwa über Quellcode, sondern fasst eine logisch und / oder funktional zusammengehörige Menge von Komponente zusammen. Über diesen Mechanismus lassen sich Hierarchien von Komponenten definieren. Zudem stellt eine Composite Component eine Abstraktion und Kapselung ihres Innenlebens (\textit{Information Hiding}) zur Verfügung, wenn sie nur über die äußeren Schnittstellen betrachtet wird (TODO: weitere Konzeptideen zu CC). In Kapitel ref{TODO Ebenen} werden die Abstraktionsebenen von Komponenten detailliert aufgezeigt.

Verfügt eine Komponente über keine weiteren inneren Komponenten, so wird diese \textit{Basic Component} genannt. Damit bildet sie das Gegenstück zu einer \textit{Composite Component}.

\textit{Composite Components} können neben \textit{Basic Components} ebenfalls weitere \textit{Composite Components} enthalten. Damit werden tiefere Hierarchiestufen realisiert. Im Beispiel ist "`Composite Component B"' eine weitere zusammengesetzte Komponente. Wie zu sehen ist, können \textit{Composite Components} genau wie andere Komponenten verwendet werden. Alle enthaltenen Komponente einer \textit{Composite Component} sind teil der "`contains"'-Relation der zusammengesetzen Komponente.

In Abbildung \ref{fig:cm-composite-component-01} wird eine exemplarische \textit{Composite Component} dargestellt. Wie zu sehen ist, kann eine \textit{Composite Component} intern eine beliebige Menge von Komponente aufnehmen. Damit diese Komponenten verwendet werden können, bedient man sich zwischen den Komponenten auf der gleichen Hierarchiestufe den bereits eingeführten Assembly Konnektoren. Zusätzlich reglementieren Delegationskonnektoren (\textit{Delegation Connector}) wie auf der Provides-Seite externe Aufrufe auf interne Aufrufe geleitet werden und umgekehrt wie auf der Requires Seite Aufrufe interner Komponente nach außen aus die \textit{Composite Component} heraus geleitet werden.

Damit die Abbildung zwischen der äußeren und inneren Schnittstelle einer Composite Component eindeutig bleibt, muss stets eine \texttt{1:1} Beziehung zwischen äußerer und innerer Schnittstelle bestehen. Ein Delegationskonnektor verbindet als immer genau zwei Schnittstellen (eine äußere angebotene Schnittstelle mit einer inneren angebotenen Schnittstelle \textit{oder} eine äußere geforderte Schnittstelle mit einer inneren geforderten Schnittstelle).

Manchmal werden Delegationskonnektoren zusätzlich über ihren Verwendungskontext unterschieden. Findet die Verwendung zum Zwecke einer Abbildung zwischen angebotenen Schnittstellen statt, spricht man auch von \textit{Provides Delegation Connectors}. Im umgekehrten Fall der Verwendung zwischen benötigten Schnittstellen wird der entsprechende Delegationskonnektor auch \textit{Requires Delegation Connector} genant.

Ein Signatur"=Mapping im Sinne eines Adapters (vgl. auch \cite{GAMMA,BUSCH}) ist für die Delagationskonnektoren jedoch nicht möglich. In diesem Falle wäre eine Adapterkomponente vorzusehen, die die notwendige Adaption vornimmt.

Da \textit{Composite Components} lediglich eine logische Kapselung bieten, lassen sie sich vollständig auflösen, sofern ihre direkt enthaltenen (in der "`contains"'-Relation enhaltenen) Komponenten vollständig einschließlich aller Delegationskonnektoren und Assembly Konnektoren bekannt sind. Zu diesem Zwecke zeigen Delegaten auf der Angebotsseite direkt auf die enthaltenen Komponenten, die über Delegationskonnektoren verknüpft sind, beziehungsweise auf der Nachfrageseite direkt auf die über Delegationskonnektoren verbundenen äußeren Komponenten. Zu beachten ist, dass Composite Components, je nach Typ-Ebene (siehe Kapitel ref{Ebenen}), nicht von anderen Komponenten unterschieden werden können und ihre interne Realisierung nicht immer vollständigs bekannt sein muss. In diesen Fällen ist eine Auflösung nicht möglich.

Auch wenn sich \textit{Composite Components} logisch auflösen lassen, so ist dies immer mit einem Informationsverlust verbunden. Da allein die Kapselung einer Menge von Komponenten eine Information darstellt, wenn man davon ausgeht, dass der Vorgang des Zusammenfassens von Komponenten nicht zufällig geschieht. Daneben ist auch der Name einer \textit{Composite Component} bezeichnend. Zudem ist es im Komponentenmodell möglich über Annotationen Informationen zu einer zusammengesetzten Komponente zu hinterlegen (siehe auch Kapitel ref{TODO: Anotationen}). Wird die Komponente aufgelöst, lassen sind auch diese Informationen nicht mehr speichern.

Nicht immer können \textit{Composite Components} jedoch aufgelöst werden. An dieser Stelle sei auf Kapitel ref{Ebenen} verwiesen. Je nach Typ-Ebene wird das Innenleben einer Komponente teils bewußt ausgeblendet und bleibt in jedem Falle verborgen. Zudem läßt das Komponentenmodell \textit{Composite Components} zu, die nicht ausspezifiziert sind. Über diesen Mechanismus lassen sich Software"=Architekturen schrittweise verfeinern und mit mehr Informationen anreichern.



\subsection{Schnittstellen, Signaturlisten und Protokolle}
\label{sec:SchnittstellenSignaturlistenundProtokolle}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/cm-interfaces-signature-protocol-01.pdf}
	\caption{Beispielschnittstellen mit Signaturlisten und Protokollinformationen in Form von endlichen Automaten}
	\label{fig:cm-interfaces-signature-protocol-01}
\end{figure}



Bisher wurden Schnittstelle nicht näher charakterisiert. Daher konnten auch keine Interoperabilitätsbedingungen für Komponenten definiert werden. Wesentliche Eigenschaften einer Schnittstelle werden durch die Signaturliste und durch das Protokoll bestimmt. Auf Schnittstellen können innerhalb von Signaturlisten Dienste definiert sein, sowie ein Protokoll, das gültige Aufrufreihenfolgen von Diensten definiert.

Abbildung \ref{fig:cm-interfaces-signature-protocol-01} zeigt eine Beispielskomponente "`ReadWriter Component"', die die Schnittstelle "`Interface 1"' anbietet und die Schnittstellen "`Interface 2"' und "`Interface 3"' benötigt. Diese Komponente wird auch in Kapitel \ref{sec:ServiceEffektSpezifikation} als Beispiel verwendet. Zudem wird zu jeder Schnittstelle eine beispielhafte Signaturliste mit dazu gehörigem Protokoll im Form eines endlichen Automaten gezeigt.



\subsubsection{Signaturlisten und Signaturen}
\label{sec:SignaturlistenundSignaturen}
Jeder Dienst weist auf einer Schnittstelle eine eindeutige Signatur auf, etwa \texttt{void} \texttt{Do\-Some\-thing(int a)}. In Anlehnung an Methoden"=Signaturen aus Programmiersprachen wie C\# und Java haben Signaturen

\begin{itemize}
	\item einen Rückgabetyp oder \texttt{void},
	\item einen Bezeichner, üblicherweise mit einem sprechenden Namen,
	\item eine Menge von Parametern (\texttt{0..*}), die jeweils aus einem Typ und einem Bezeichner (innerhalb der Parameter eindeutig) bestehen. Zusätzlich können die Modifizierer \texttt{in}, \texttt{out} und \texttt{ref} für Parameter in Analogie zur C\# Semantik verwendet werden. Parameter werden in Klammern angegeben und durch Kommata getrennt, Modifizierer werden den Parametern vorangestellt.
	\item Daneben müssen alle \textit{Exceptions} (Ausnahmen) angegeben werden, die von einer Signatur geworfen werden können. Die verpflichtende Angabe von \textit{Exceptions} folgt den Vorgaben von Java. \textit{Exceptions} werden mit dem Schlüsselwort \texttt{throws} an die Methoden"=Signatur angehängt und durch Kommata getrennt.
\end{itemize}

Eine Signatur ist eindeutig über Rückgabetyp, Bezeichner und Parameter (Typ und Bezeichner) unter Berücksichtigung der Reihenfolge.



Eine Schnittstelle enthält genau eine Signaturliste. Diese Signaturliste wiederum enthält \texttt{0..*} Signaturen. Signaturen und Signaturlisten können nicht zur gleichen Zeit von verschiedenen Schnittstellen referenziert werden. Gleichwohl können verschiedene Schnittstellen identische Signaturlisten oder einzelne Signaturen definieren. 



\subsubsection{Protokolle}
\label{sec:Protokolle}
Wie bereits oben angedeutet, definieren Protokolle von Schnittstellen gültige Aufrufsequenzen auf Diensten. In Abbildung \ref{fig:cm-interfaces-signature-protocol-01} werden beispielhaft endliche Automaten zur Darstellung der Protokolle auf den Schnittstellen verwendet. Das Komponentenmodell limitiert dabei Protokolle nicht auf einen bestimmen Typ, wie endliche Automaten oder Petri"=Netze, sondern lässt diese Entscheidung bewußt offen. Schnittstellen müssen nicht zwangsläufig Protokolle definieren.

Um im Beispiel zu bleiben: hier beginnen alle gültigen Aufrufsequenzen mit einem \texttt{open()}, gefolgt von beliebigen \texttt{read()} und \texttt{write()} Operationen. In jedem Fall muss abschließend ein \texttt{close()} folgen. Alternativ sind auch Sequenzen gültig, die sofort enden (denn direkt nach dem Initialisieren befindet sich das Protokoll der Schnittstelle in einem gültigen Endzustand). Als Kantenbeschriftungen werden also Signaturen aus der Signaturliste der mit einem Protokoll zu versehenden Schnittstelle verwendet.

Protokolle bieten somit eine Möglichkeit Abhängigkeiten zwischen Diensten, respektive Signaturaufrufen, \textit{einer} Schnittstelle definieren. Damit wird für Komponenten ein interner Zustand möglich. Über Dienstaufrufe kann der Zustand verändert werden. Abhängig von aktuellen Zustand sind nur bestimmte weitere Dienstaufrufe möglich. Nur bestimmte Sequenzen, nämlich jene, die zu einem Endzustand führen, sind gültig. Von Komponenten kann nur dann ein definiertes Verhalten erwartet werden, wenn die Schnittstellenprotokolle erfüllt werden.



\paragraph{Einschränkungen}
\label{sec:EinschraenkungenProtokoll}
Das Komponentenmodell erlaubt indes derzeit keinen Zustandswechsel in Abhängigkeit von Aufrufparametern einer Signatur. Um weitere Zustandswechsel einer Komponente modellieren zu können, ist es notwendig die Zahl der Signaturen zu erhöhen. Damit kann, bei endlichen Parameterwerten, ein Verhalten nachgebildet werden, dass einer Abhängigkeit des internen Komponentenzustands von Parameterwerten entspricht.

Zu beachten ist, dass Schnittstellen ein Protokoll definieren können, der Zustand jedoch von der implementierenden und anbietenden (\textit{provides}) Komponente abhängt.

Als weitere Einschränkung des Komponentenmodells gilt derzeit, dass eine Komponente mehrere Schnittstellen zugleich anbieten oder benötigen können. Innerhalb der Menge der angebotenen \textit{oder} benötigten Schnittstellen gibt es jedoch keine Möglichkeit schnittstellenübergreifende Protokolle zu definieren. Somit kann eine Komponente zur gleichen Zeit \textit{mehrere} interne Zustande haben. Je Schnittstellenprotokoll existiert ein eigener Zustand.



\paragraph{Schlechte Modellierung}
\label{sec:SchlechteModellierung}
Sind die Signaturlisten und Protokolle, die auf verschiedenen Schnittstelle definiert wurden, identisch, deutet dies zumeist auf ein Design"=Problem des Komponentenmodells hin. Sind auch Zusatzattribute (z. B. QoS"=Attribute; siehe Kapitel ref{TODO Anotationen}) identisch, führt dies vermutlich zu ungewollten Inkonsistenzen zwischen einer vermeintlich identischen Schnittstelle.



\subsection{Service Effekt Spezifikation}
\label{sec:ServiceEffektSpezifikation}



\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/cm-code-seff-01.pdf}
	\caption{Service Effekt Spezifikation als endlicher Automat zu beispielhaftem Pseudo"=Quellcode}
	\label{fig:cm-code-seff-01}
\end{figure}



In Abbildung \ref{fig:cm-code-seff-01} wird eine Service Effekt Spezifikation zu der in Abbildung \ref{fig:cm-interfaces-signature-protocol-01} eingeführten Komponente dargestellt.



------------------------------------


TODO:
- Parametrisierte Verträge: Auswirkungen auf Requires-Schnittstelle: nicht mehr alles notwendig
- Komponente ruft sich selbst auf: wann unendliche rekursion, wann auf welcher ebene erlaubt --> insbesondere unbekannt in CCs
- mehrere konnektoren auf eine provided schnittstelle: zustand nicht mehr unterscheidbar.: In der Umkehrung kann eine angebotene Schnittstelle jedoch über \texttt{0..*} benötigte Rollen verwendet werden. Damit bleibt die graphische Notation nicht eindeutig...
- keine Unterstützung für Zustände über mehrere schnittstellen hinweg.
- Typ-Ersetzbarkeit: Verschiedene Betrachtungsmöglichkeiten / Interoperabilitätsniveaus: Signatur, Protokoll, QoS --> Grafik
Um entscheiden zu können, ob Komponenten mit ihren Schnittstellen durch andere substituiert (Sub"=Typ"=Beziehung) werden können, ist es sinnvoll Protokolle, respektive Sprachen, zu verwenden, deren Inklusionsproblem entscheidbar ist.
- Anotationen wann und wo?
- QoS-Fähigkeiten


------------------------------------



Das Palladio Komponentenmodell -- unter \cite{BECK} findet sich die Beschreibung einer älteren Fassung des Modells -- beschreibt Software-Architekturen als eine Menge von Komponenten und Schnittstellen, sowie darauf definierten Relationen. Komponenten werden dabei in \textit{Basic Components} und \textit{Composite Components} unterschieden. Wie die Namen bereits andeuten, unterstützt das Komponentenmodell zusammengesetzte Strukturen. Das bedeutet, dass Komponenten und Schnittstellen mit beliebiger Tiefe rekursiv in \textit{Composite Components} geschachtelt werden können. In diesem Falle läßt sich die "`enthält"' Relation als Realisierung einer Komponente interpretieren. Das bedeutet, dass Komponenten, die innerhalb einer \textit{Composite Component} liegen, das Verhalten der äußeren Komponente bestimmen.

Komponenten können Schnittstellen anbieten (\textit{Provides Interfaces}) oder benötigen (\textit{Requires Interfaces}). Auf Schnittstellen sind dabei Dienste definiert, etwa \texttt{void} \texttt{Do\-Some\-thing(int a)}, sowie ein Protokoll, das gültige Aufrufreihenfolgen von Diensten definiert. Das Komponentenmodell limitiert dabei Protokolle nicht auf einen bestimmen Typ, wie endliche Automaten oder Petri"=Netze, sondern lässt diese Entscheidung bewußt offen. Um entscheiden zu können, ob Komponenten mit ihren Schnittstellen durch andere substituiert (Sub"=Typ"=Beziehung) werden können, ist es sinnvoll Protokolle, respektive Sprachen, zu verwenden, deren Inklusionsproblem entscheidbar ist.

Eine gültige Substitution für eine Komponente liegt im Allgemeinen dann vor, wenn auf allen \textit{Provides Interfaces} einer Komponente
\begin{itemize}
	\item die Menge der angebotenen Dienste eine Obermenge der geforderten Dienste ist, und
	\item das angebotene Protokoll eine Sprach-Obermenge des geforderten Protokolls ist.
\end{itemize}
Umgekehrt, man spricht in diesem Fall von "`Contra-Varianz"' (siehe auch \cite{co-contra}), muss für alle \textit{Requires Interfaces} gelten, dass
\begin{itemize}
	\item die geforderten Dienste eine Untermenge der angebotenen Dienste sind, und
	\item das geforderte Protokoll eine Sprach-Untermenge des angebotenen Protokolls ist.
\end{itemize}

Daneben wird die Komponenten- und Schnittstellen"=Struktur in verschiedene Ebenen unterteilt, worüber sich Typ"=Konformitäts"=Anforderungen für Komponenten definieren lassen. Auf der obersten Ebene wird vom \textit{Provided Type} verlangt, zu deklarieren, welche Schnittstellen angeboten werden. Darunter liegende Ebenen verfeinern im Allgemeinen unter Erhaltung der Typ"=Konformität den Typ einer höheren Ebene. Über diesen Mechanismus lassen sich Software-Architekturen schrittweise verfeinern und mit mehr Informationen anreichern.

Zusätzlich zum \textit{Provided Type} definiert das Komponentenmodell den \textit{Complete Type} und den \textit{Implementation Type}. Auf der Ebene des \textit{Complete Types} wird von Komponenten verlangt alle geforderten Schnittstellen (\textit{Requires Interfaces}) zu deklarieren. Unterhalb des \textit{Complete Types} liegt der \textit{Implementation Type}, der zusätzlich zur Schnittstellendeklaration Informationen über seine Realisierung enthält.

Damit einher geht die Definition der \textit{Service Effect Specification, SEFF}. Diese gibt an, welche externen Effekte ein Dienstaufruf auf einer Komponente hat. Zu jedem angebotenen Dienst existiert daher eine Beschreibung, in welchen Aufrufen von geforderten Diensten die Verwendung einer Komponente resultiert. Da die Verwendung von geforderten Diensten vom Zustand einer Komponente oder Aufrufparametern eines Dienstes abhängen kann, werden \textit{SEFFs} beispielsweise als endliche Automaten dargestellten. Damit können auch Endscheidungen (beispielsweise \texttt{if}-Ausdrücke) oder Schleifen (etwa \texttt{while}) erfasst werden. Auch hier ist das Komponentenmodell nicht auf die konkrete Ausprägung in Form endlichen Automaten limitiert. Ebenso sind Petri"=Netze oder ähnliche Sprachdefinitionen denkbar.

Neu wird im Palladio Komponentenmodell der Begriff des "`Kontexts"' eingeführt. Dabei wird unterschieden in
\begin{itemize}
	\item "`Verdrahtung"' von Komponenten untereinander im Sinne einer Komponentenarchitektur (\textit{System Construction / Assembly Composition}) und
	\item Allokation von Komponenten auf Ressourcen (\textit{Deployment / Allocation}).
\end{itemize}
Das Ziel des dahinter liegenden Konzepts ist eine Möglichkeit zur Unterscheidung von Komponenteneigenschaften je nach ihrem konkreten Verwendungskontext. So kann die \textit{gleiche} Komponente, unterschieden nach dem Typ auf einer der oben genannten Ebenen, ein unterschiedliches Verhalten je nach Kontext aufweisen.

Nach der \textit{Assembly Composition} richten sich die aufgerufenen und aufrufenden Dienste. Damit können z. B. die Aufrufparameter von Diensten variieren. Es ändern sich also primär funktionale Eigenschaften einer Komponente. Mit der \textit{Allocation} variiert unter anderem die Ausführungsumgebung. Dies hat z. B. Auswirkungen auf die Ausführungsgeschwindigkeit, Sicherheit oder die Konfiguration zur Ausführung einer Komponente. Vornehmlich sind hiervon also nicht"=funktionale Eigenschaften betroffen.

Zusätzlich zu den beschriebenen statischen Strukturen bietet das Komponentenmodell Unterstützung für beliebige Zusatzattribute, die für Entitäten (Schnittstellen und Komponenten, Protokolle u. a.) vergegeben werden können. Hierunter fallen ebenfalls QoS (\textit{Quality of Service}) Attribute, für die das Komponentenmodell eine Berechnungsgrundlage bietet. Das Komponentenmodells ermöglicht mit seinen Daten die Auswertung von parametrisierten Verträgen, wie sie von Reussner in \cite{reussner01i} beschrieben werden.

Ein neues Paper, das das aktuelle Komponentenmodell beschreibt, wird in Kürze unter \cite{PalladioCMNeu} veröffentlicht.



---------------------------------




\subsection{Eclipse}
\label{sec:Eclipse}
Eclipse, wie auch alle anderen zu verwendenden Werkzeuge (zumeist Eclipse"=Plugins) im Bereich der Meta-Modellierung und anschließenden Modell-Transformation, befinden sich in einer stetigen Entwicklung. Der Charakter von Open-Source-Projekten macht die Entwicklungsarbeit in Teilen vom Engagement freiwilliger Entwickler ab\-hän\-gig. Dies hat zur Folge, dass die Wachstumsgeschwindigkeit dieser Projekte stark variabel sein kann, die Erweiterungen zugleich aber auch neue Möglichkeiten bieten können, die zum Zeitpunkt der Erstellung dieses Propsals nicht absehbar sind.

Ebenso sind möglicherweise nicht alle verfügbaren Produkte bekannt, die sich zur Umsetzung der Diplomarbeit eignen würden. Somit kann die Auswahl geeigneter Werkzeuge lediglich eine unvollständige  Momentaufnahme in der Marktbeobachtung darstellen. Änderungen in der Verfügbarkeit von Werkzeugen haben somit direkt Einfluß auf die Diplomarbeit, sind zugleich aber nicht vorhersehbar.

Eine ausführliche Abschätzung zu den allgemeinen Risiken findet sich in Kapitel \ref{sec:Risikomanagement}.



\subsubsection{Entwicklung des Eclipse-Projekts}
\label{sec:EntwicklungdesEclipse-Projekts}
Eclipse \cite{eclipse} stellt ein umfangreiches Open Source Framework unter Java dar. Als solches stellt Eclipse diverse Plugin-Schnittstellen zur Verfügung, für die bereits eine Vielzahl Plugins existieren.

Während das primäre Ziel von Eclipse die Bereitstellung einer Entwicklungsumgebung ist, wurden daneben mehrere Unterprojekte ausgegliedert, die sich des Eclipse"=Frameworks bedienen und für MDA-Prozesse (\textit{Model Driven Architecture}) Un\-ter\-stüt\-zung bereit stellen.

Das Eclipse-Projekt wird maßgeblich durch IBM \cite{IBM} gefördert. So basieren auch neuere kommerzielle Entwicklungen wie der IBM Rational Software Architect 6 oder Borland Together 2006 auf dem Eclipse-Framework. Dies verdeutlicht die Bedeutung Eclipses für industrielle Anwendungen, führt andererseits aber auch dazu, dass sich die genannten Firmen, neben weiteren, finanziell und personell für das Eclipse-Projekt engagieren. Somit ist zugleich eine Weiterentwicklung des Projekts möglich, die Ideen und Anregungen aus der Open Source Community aufgreift und mit den Mitteln der Industrie umsetzt.

Startete das Eclipse-Projekt im November 2001 noch mit acht Gründungsmitgliedern, waren es Ende 2003 bereits 80 und heute bereits über 115 Mitglieder (vgl. \cite{eclipse-history}). Das anhaltende Wachstum und die Diversifikation über mehrere Sub-Projekte erscheint somit auch für die Zukunft gesichert.



\subsubsection{Eclipse Sub-Projekte}
\label{sec:EclipseSub-Projekte}
Im Rahmen der Diplomarbeit werden unter anderem die folgenden Sub-Projekte der Eclipse Foundation verwendet: EMF - Eclipse Modeling Framework \cite{EMF}, GEF - Graphical Editing Framework \cite{GEF}, GMF - Graphical Modeling Framework \cite{GMF} und der Merlin Generator \cite{Merlin}.



\subsubsection{Verwendung im Palladio-Kontext}
\label{sec:VerwendungimPalladio-Kontext}
Die DFG Nachwuchsgruppe Palladio \cite{PALL}, in deren Rahmen diese Diplomarbeit entsteht, entwickelt seit dem Jahr 2003 unter anderem ein Komponentenmodell zur ingenieursmäßigen Evaluation von Software"=Architekturen, zur Vorhersage nicht"=funktionaler Eigenschaften und Untersuchung von Modelltransformationen (siehe auch Kapitel \ref{sec:DasPalladioKomponentenmodell}). Zur Validierung wissenschaftlicher Erkenntnisse wurde neben der konzeptionellen Entwicklung stets eine Referenz"=Implementierung des Palladio Komponentenmodells gepflegt. Erschien die Entwicklung in der Vergangenheit unter Microsofts .NET sinnvoll, vor allem weil die verwendete Programmiersprache C\# 1.0 gegenüber Java 1.4 benötigte Spracherweiterungen bot (bspw. Klassen"=Attribute), so zeigt bereits die in Kapitel  \ref{sec:EclipseSub-Projekte}, aufgeführte Palette existierender Eclipse-Entwicklungen, dass für den ebenfalls forschungsrelevanten Bereich der Modelltransformationen eine weitaus breitere Unterstützung unter Eclipse existiert. Zudem ist das in Java 1.4 als einschränkend empfundene Fehlen von Klassen"=Attributen unter Java 1.5 aufgehoben, da Java mittlerweile ähnliche Konzepte bietet.

So sprechen folgende Argumente für die Verwendung von Eclipse:

\begin{itemize}
	\item Das Basis-Framework Eclipse stellt die Möglichkeit bereit, eigene Plugins zu entwickeln um benötigten Funktionsumfang zu ergänzen. Daneben kann Eclipse als Basis für eigene selbständige Applikationen dienen (Rich Client Platform \cite{rcp}).
	\item Insbesondere EMF (eine detailliertere Erklärung zu den hier aufgeführten Eclipse"=Projekten findet sich in Kapitel \ref{sec:EMF}) scheint als zentrales Daten"=Modell zur Beschreibung des Palladio Metamodells (des Komponentenmodells) geeignet. Zudem bietet EMF die Möglichkeit generativ Modell"=Quellcode, Queries und weiteres (siehe Kapitel \ref{sec:Gesamtprozess}) zu erzeugen.
	\item Daran anschließend erleichtern Modelltransformatoren wie Merlin und GMF die Erzeugung eines graphischen GEF-Editors.
	\item Schließlich stellt Eclipse mit GEF ein umfangreiches GUI-Framework bereit. Die Verwendung von GEF in kommerziellen Produkten wie Borland Together Software Architect 2006 und IBM Rational Architect 6 zeigt die praktische Relevanz von GEF.
\end{itemize}

Die .NET-Umgebung liegt in all diesen Bereichen deutlich zurück. Es gibt keine freien Entsprechungen zu EMF, Modelltransformationstools oder freien Grafikbibliotheken mit der Mächtigkeit der Eclipse Sub"=Projekte. Der Wechsel von .NET zu Java und Eclipse erscheint unter diesen Gesichtspunkten sinnvoll:

\begin{itemize}
	\item Die Möglichkeit auf die Generierung von Java-Quellcode zu einem Meta-Modell zurückzugreifen verspricht einen deutlichen Zeitgewinn, insbesondere bei einem häufigen Wechsel des zu Grunde liegenden Meta-Modells. Die Erfahrung innerhalb der Palladio"=Gruppe verdeutlicht, dass Änderungen des Komponentenmodells häufig vorgenommen wurden und werden. Dies ist wenig verwunderlich, zumal das Komponentenmodell zentraler Gegenstand der Forschung der Palladio"=Gruppe ist.
	\item GEF stellt ein umfangreiches Framework zur graphischen Anzeige und Bearbeitung von Graphiken bereit. Das Framework ist erweiterbar und verfügt über umfangreiche Layout- und Rendering"=Werkzeuge. Die Architektur folgt dabei dem Model"=View"=Controller"=Prinzip. GEF ist domänenneutral und daher für eine breite Palette von Applikationen geeignet. Standardmäßig werden bereits Aktivitätsdiagramme, GUI"=Builder, Klassendiagramm"=Editoren, Zustandsmaschinen und WYSIWYG Text-Editoren unterstützt.
	\item GMF und Merlin verfolgen einen generativen Ansatz, um zu bestehenden EMF-Meta-Modellen GEF-Editoren zu erzeugen. Merlin beschränkt sich dabei nicht nur auf das Generieren von GEF-Editoren, sondern unterstützt über JET Templates selbst definierte Transformationen (siehe Kapitel \ref{sec:Merlin-GMF}). Dagegen versteht sich GMF als "`generative Brücke zwischen EMF und GEF"' und zielt speziell auf die Erzeugung mächtiger, flexibel generierbarer Editoren. Siehe hierzu auch Kapitel \ref{sec:Merlin-GMF}.
\end{itemize}



\subsection{MOF und EMF}
\label{sec:MOF}
\label{sec:EMF}
MOF, die Meta Object Facility \cite{MOF-Spezifikation} der OMG, bietet eine Standard-Schnittstelle für Meta-Modellierung an. Entsprechend der Philosophie von MDA (\textit{Model Driven Architecture}, \cite{MDA}), ist MOF auf Plattformunabhängigkeit und Wiederverwendbarkeit bedacht. Der als Framework ausgelegte Standard beinhaltet Mechanismen zum Erzeugen, Manipulieren, Finden, Ändern und Zerstören von Objekten und Beziehungen zwischen diesen Objekten. Die Menge gültiger Aktionen wird dabei über Meta-Modelle festgelegt.

In der aktuellen Spezifikation MOF 2.0 wird in Essential MOF (EMOF) und Complete MOF (CMOF) unterschieden. EMOF zielt dabei primär auf eine Erleichterung der Unterstützung durch Tools. Dazu wurde EMOF im Funktionsumfang reduziert und beschränkt sich auf die Kernkonzepte von MOF (\textit{Capabilities}): \textit{Reflection}, \textit{Identifiers}, \textit{Extension} und \textit{PrimitiveTypes}. Hinter \textit{Reflection} verbirgt sich ein Mechanismus zum Erkennen und Manipulieren von Meta-Objekten und Meta-Daten ohne initiales Wissen über Objekte, ähnlich dem Reflection-Mechanismus aus Microsofts C\#. \textit{Identifier} sind derzeit als URIs (Uniform Resource Identifier) realisiert und garantieren eine eindeutige Identifikation von Objekten, die auch über Transforms und Serialisierungen hinweg möglich ist. \textit{Extensions} stellen eine Möglichkeit bereit, Objekte dynamisch zu anotieren und unvorhersehbare Informationen über Key-Value-Paare zu ergänzen. \textit{PrimitiveTypes} stellen schließlich einen Basissatz von primitiven Datentypen (etwa String, Integer) zur Verfügung.

EMF, das Eclipse Modeling Framework \cite{EMF}, ist in der aktuellen Version eine Java-Implementierung des EMOF-Standards, mit nur kleineren Abweichungen insbesondere bei der Benennung von Konzepten. EMF steht als Eclipse Plugin zum freien Download bereit. Das Basis-Datenmodell von EMF ist ECORE. Die Nähe zwischen EMOF und EMF geht so weit, dass EMF EMOF transparent lesen und schreiben kann.

Für die Diplomarbeit bietet EMF den Vorteil, dass es sich nahtlos in Eclipse integriert, zugleich aber die Implementierung des weit verbreiteten EMOF-Standards ist. Damit bleibt die Kompatiblität zu anderen EMOF-Implementierungen erhalten, was später einen einfachen Datenaustausch ermöglichen kann.



\subsection{Modellierung und Transformation}
\label{sec:ModellierungundTransformation}



\subsubsection{Transformation}
\label{sec:Transformation}
Transformationen werden in drei Klassen unterschieden:
\begin{itemize}
	\item \textbf{Model-to-Model} Werden zwei Modelle in einander überführt, so spricht man von einer Model-to-Model Transformation. Der Import von ECORE-Modellen in EMF entspricht einem solchen Transformationsvorgang.
	\item \textbf{Model-to-Text} Werden Modelle auf eine textuelle ("`flache"') Repräsentation transformiert, so spricht man von Model-to-Text. Die Verwendung von JET (\textit{Java Emitter Templates}) in Merlin ermöglicht genau diese Form von Transformation. Mit Hilfe von JET-Templates wird zu einem gegebenen Modell eine GEF-Editor erzeugt. Auch GMF bietet eine Form von Model-to-Text Transformation, indem ebenfalls zu einem gegebenem Modell ein GEF-Editor generiert wird.
	\item \textbf{Text-to-Model} Unter Text-to-Model kann der Importvorgang von Quellcode mit anschließender Überführung in ein Modell verstanden werden. Im Sinne eines Roundtrip-Zyklus zwischen Quellcode und Modell werden Modellinformationen aus Quellcode extrahiert.
\end{itemize}
Die Transformation von Text zu Text ist im Rahmen der Diplomarbeit nicht von Bedeutung. Anhand des Prozesses, der in Kapitel \ref{sec:Gesamtprozess} dargestellt wird, lassen sich die unterschiedlichen Transformationsvorgänge leicht nachvollziehen.



\subsubsection{Merlin / GMF}
\label{sec:Merlin-GMF}
Der Merlin-Generator \cite{Merlin} ist ein freies Eclipse Plugin, das auf EMF basiert. Dem MDA-Ansatz folgend, ermöglicht Merlin die Definition erweiterter Mappings für Java Templates (JET) und Model-to-Model Transformationen, wie zum Beispiel in ECORE, von wo aus eine Quellcode-Generierung angestoßen werden kann.

GMF, das Graphical Modeling Framework \cite{GMF}, versucht, wie auch Merlin, die Brücke zwischen EMF und GEF zu schlagen. Hinter GMF stehen Branchengrößen wie IBM und Borland, die das Projekt mit professionellen Mitarbeitern fördern. Ziel des Projekts ist die Schaffung einer gemeinsamen Infrastruktur für EMF-basierte Modelle zur Erzeugung GEF-basierter GUIs.



\section{Konzeption}
\label{sec:Konzeption}
Die Idee hinter der Diplomarbeit ist die Möglichkeit, einen GUI-Editor nur auf Basis eines Meta-Modells sowie detaillierter Transformationsanweisungen erzeugen zu können. Änderungen am Meta-Modell könnten sich somit, solange sich die Änderungen in einem gewissen Rahmen bewegen, rein über einen neuerlichen Generierungsprozess in einem GUI-Editor wiederfinden. Damit würden Modifikationen schnell und einfach möglich.

Damit die skizzierte Idee sinnvoll durchgeführt werden kann, darf jedoch der Aufwand (zeitlich und personell) für den Durchlauf des generativen Ansatzes nicht den Aufwand einer manuellen Implementierung überschreiten. Der Break-Even-Point ergibt sich demnach aus den folgenden Faktoren:

\begin{itemize}
	\item Generativer Ansatz	
	\begin{enumerate}
		\item Initiale Erstellung des Meta"=Modells
		\item Pflege des Meta-Modells
		\item Initiale Definition von Transformationen
		\item Anpassung der Transformationen an die eigenen Bedürfnisse (\textit{customizing})
		\item Pflege der Transformationsanweisungen
		\item Häufigkeit der Änderungen des Meta-Modells
		\item "`Technologietreue"'; wie häufig werden die verwendeten Techniken gewechselt?
		\item "`Intensität"' der Änderungen; liegt ein Paradigmenwechsel vor, der die Anpassung von Transformationen verhindert?
		\item Aufwand für die Durchführung eines Generierungsdurchlaufs
		\item Aufwand für die "`manuelle Nachpflege"' eines Generierungsdurchlaufs
	\end{enumerate}
	\item Manuelle Implementierung
	\begin{enumerate}
		\item Häufigkeit der Änderungen des Meta-Modells
		\item Aufwand der Code-Pflege nach einer Modifikation des Meta"=Modells
		\item "`Technologietreue"'; wie häufig werden die verwendeten Techniken gewechselt?
		\item Aufwand für die Anpassung existierender Frameworks, die verwendet werden
	\end{enumerate}
\end{itemize}

Ein manuell erzeugter Editor dürfte dabei zumeist den Vorteil haben, flexibler anpassbar zu sein und den eigenen Visionen und Bedürfnissen einer Editor-Architektur folgen zu können. Spezielle eigene Funktionen können einem manuell erzeugten Editor einfacher hinzugefügt werden, da dazu keine Templates angepasst werden müssten. Der Aufwand bei einem Technologie"=Wechsel ist wahrscheinlich kleiner als im Vergleichsfall des generativen Ansatzes, da umfangreichere Änderungen in der Architektur ohnehin berücksichtigt werden müssten.

Den größten Einfluß auf die Entscheidung, den manuellen Ansatz oder den generativen Ansatz zu wählen, hat die Häufigkeit der Änderungen am Meta-Modell und damit verbunden der Aufwand, einen generierten Editor manuell nachzupflegen.

Im Rahmen eingehender Überlegungen in der Palladio"=Gruppe im Vorfeld der Diplomarbeit und extrapoliert aus den Erfahren mit dem Komponentenmodell in der Vergangenheit, scheint ein generativer Ansatz eine Aufwandsverminderung zu bedeuten. Eine Evaluation erfolgt somit en passant mit der Diplomarbeit.



\subsection{Geplanter Prozess}
\label{sec:GeplanterProzess}
In den folgenden beiden Unterkapiteln wird der für die Diplomarbeit geplante Prozess beschrieben. In Kapitel \ref{sec:Gesamtprozess} wird der Gesamtprozess beschrieben, der vom Ablauf der Modellierung des Komponenten-Meta-Modells über Transformationen bis hin zu einem lauffähigen GEF-Editor geht. Den Kern der Diplomarbeit bildet hingegen der in Kapitel \ref{sec:Modellierungsprozess} beschriebene Prozess: die Modellierung des Meta-Modells des Palladio Komponentenmodells in EMF.



\subsubsection{Gesamtprozess}
\label{sec:Gesamtprozess}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/prozess01.pdf}
	\caption{Skizze des Gesamt-Entwicklungsprozesses der Diplomarbeit zur Generierung eines GEF-Editors}
	\label{fig:prozess01}
\end{figure}

In Abbildung \ref{fig:prozess01} wird der angestrebte Entwicklungsprozess für die Diplomarbeit grob skizziert. Im Kern wird sich die Diplomarbeit dabei mit den im oberen Bereich der Abbildung dargestellten Teilen des Entwicklungsprozesses beschäftigen, der Erzeugung eines Meta-Modells des Palladio Komponentenmodells.

Dabei läßt sich der Entwicklungsprozess in die folgenden Schritte gliedern (wobei Tools in blau, respektive dunkel und Artefakte in gelb, respektive hell in Abbildung \ref{fig:prozess01} dargestellt sind):

\begin{enumerate}
	\item Für die initiale Beschreibung des Meta-Modells in UML 2 bieten sich verschiedene Alternativen an. Zum einen ist eine Modellierung mit Hilfe von IBM Rational Software Architect 6 denkbar, daneben ist eine Modellierung unter Borland Together Architect 2006 möglich. In beiden Fällen ist ein Export zu UML2 und ECORE möglich. Da beide Tools einen ähnlichen Funktionsumfang aufweisen und gleichermaßen in Eclipse integriert sind, dürfte die Art der Verfügbaren Lizenzen über die Verwendung entscheiden. Derzeit existiert für IBM Rational eine Lizensierung für das OFFIS \cite{OFFIS}, die im Rahmen der Arbeit verwendet werden kann.
	\item\label{enum:ECORE} Im zweiten Schritt liegt das Meta-Modell, wie bereits beschrieben, alternativ als ECORE- oder UML2-Modell vor. Parallel dazu besteht die Option anotiertes Java für den folgenden Import in EMF zu verwenden. An dieser Stelle erscheint derzeit die Verwendung von ECORE den anderen zu bevorzugen, da ECORE das Basis-Meta-Modell für EMF darstellt.
	Eine Nutzung von anotiertem Java ist initial ohnehin nicht möglich, da dies zuvor eine einmalige Generierung von Java-Code voraussetzt (siehe hierzu auch Schritt \ref{enum:RoundtripZyklus}).
	\item Von zentraler Bedeutung für die Konzeption des Modellierungs-Prozesses ist die Verwendung von Eclipse, erweitert um EMF als Modellframework. Als Eingaben bietet sich neben den bereits genannten Möglichkeiten in Form von ECORE, UML2 und anotiertem Java auch das Serialisierungsformat von IBM Rational an. Aus Gründen der Kompatiblität und der Offenheit für den Datenaustausch mit anderen Modellierungstools wird das Rational Datenformat jedoch nicht berücksichtigt, da es sich um ein proprietäres Datenformat handelt.
	\item Das Kernmodell für das zu entwickelnde Meta-Modell soll eine EMF-Repräsentation darstellen. In der Abbildung \ref{fig:prozess01} wird dieses Artefakt daher auch mit fetten Lettern hervorgehoben. Insbesondere durch die Möglichkeit EMF im Rahmen eines Roundtrip-Prozesses einzubinden, wie er in den folgenden Schritten skizziert wird, zeigt sich die Eignung als Kerndatenmodell, das, wie genannt, über viele Wege durch Imports aufgebaut und durch die Nähe zum EMOF-Standard weiterverwendet werden kann.
	\item Der mit \textit{EMF"=Generator} bezeichnete Kasten des Prozesses stellt einen wichtigen Aspekt für die Möglichkeit dar, ein Meta-Modell-Framework zu einem gegebenem EMF"=Modell zu generieren. Der "`EMF"=Generator"' setzt sich intern aus den Paketen \textit{core Framework} und \textit{EMF.Edit} zusammen.
	\item Insgesamt wird über den EMF-Generator ein vollwertiges Eclipse"=Plugin erzeugt, dass das vorgegebene Meta"=Modell bearbeitbar macht. Mit Hilfe des EMF"=Generators lassen sich die folgenden Teile eines Code"=Frameworks zu einem gegebenen Meta"=Modell generieren:
	
	\begin{itemize}
		\item \textbf{Klassen} Zu allen Elementen eines Meta-Modells kann Java-Quellcode erzeugt werden. Dabei werden alle in den UML-Diagrammen angegebenen Attribute in Getter / Setter übersetzt und Methoden angelegt. Der Generator erzeugt zu allen Meta-Modell-Entitäten eigene Interfaces sowie eine dazugehörige Implementierung. Auch manuelle Änderungen werden von diesem Generator berücksichtigt. Dazu wird für generierten Quellcode das \textit{Tag} \texttt{@generated} eingeführt, dass eben diesen generierten Code kennzeichnet. Wird das \textit{Tag} weggelassen, so bleiben die entsprechenden Code-Teile (bspw. nach manuellen Änderungen) erhalten, werden also nicht neu generiert.
		
		Die generierten Klassen ermöglichen automatisch eine konsistente Verwaltung von externen Referenzen. Hat also beispielsweise ein Auto eine Referenz auf seine Reifen, so können die Reifen gleichermaßen eine Referenz auf das Auto haben. Werden Änderungen an den Referenzen auf einer Seite vorgenommen, etwa das Entfernen der Zuordnung eines Reifens zu einem Auto, so sorgt ein Event-Mechanismus dafür, dass sowohl Reifen als auch Auto aktualisiert werden.
		
		Im Vergleich zu der Implementierung des Palladio Komponentenmodells im November 2005 (C\#-Implementierung, siehe \cite{PalladioCMTechreportMH}) kann damit die Auftrennung in ein relationales Schema unter Benutzung von IDs (eindeutige Bezeichner) zur Abbildung der Entitäten-Relationen und parallel dazu einer Sammlung aller vorhandenen Entitäten entfallen. Vormals konnten die Entitäten nur über einen speziellen Query-Mechanismus mit Hilfe ihrer ID erreicht werden.
		\item \textbf{Factories} Entsprechend dem Factory-Method Muster, wie es in \cite{GAMMA} beschrieben wird, wird zu jedem Modell über den EMF"=Generator ein Factory"=Interface nebst zugehöriger Implementierung generiert. Die Instanz der Factory wird dabei als Singleton (siehe ebenfalls \cite{GAMMA}) gehalten, womit garantiert wird, dass zu keinem Zeitpunkt Instanzen des Modells unter Nutzung verschiedener Instanzen der gleichen Factory erzeugt werden können.
		
		Die Kapselung des Erzeugungsprozesses (Instanzierung der Modell-Klassen) wird zugleich gekapselt und dadurch einfacher austauschbar. Die Factory liefert lediglich Instanzen des Interfaces, jedoch nicht der internen Implementierungen zurück. Sollen andere Implementierung verwendet werden, muss lediglich die einmalige Factory-Instanzierung auf eine andere Implementierung gelenkt werden.
		\item \textbf{Model Wizard} Der \textit{Model Wizard} ermöglicht es, mit den in Eclipse üblichen Dialog-Wizards neue Modelle anzulegen. Damit ist die Bedienbarkeit konsistent zu den Eclipse-Standards.
		\item \textbf{Editor} Auf der GUI-Seite besteht der generierte Quellcode im Kern aus einem modell"=spezifischen Editor. Dieser integriert sich für die im Model Wizard generierten Modelle in Eclipse und kann dann zum Bearbeiten der Modelle verwendet werden. Der Editor besteht im Wesentlichen aus einer Baumansicht (\textit{Tree View}), in der kontextsensitiv Knoten eingehängt werden können. Das heisst also, dass beispielsweise in einem Auto-Modell-Editor nur maximal vier Reifen zu einem Auto hinzufügbar sind.
		
		Neben der Baumansicht lassen sich zudem die Eigenschaften der Modell-Entitäten im \textit{Propertysheet} von Eclipse bearbeiten. Dies sind im Allgemeinen die Modell"=Eigenschaften, die über Getter / Setter verfügen.
		\item \textbf{Tests} Zu dem generierten Eclipse-Plugin werden automatisch Unit-Tests erzeugt, die im JUnit-Framework \cite{JUnit} ablauffähig sind. Diese Tests umfassen das Erzeugen von Modell-Entitäten sowie das Laden und Speichern eines Modell-Beispiels mit dem Editor-Plugin. Vor allem manuelle Anpassungen des generierten Codes werden dadurch fehlerrobuster möglich. Durch ein permanentes Testen gegen die generierten Tests wird sichergestellt, dass wichtige Grundfunktionalitäten stets erfüllt sind.
		\item \textbf{Commands} Damit zu einem Zeitpunkt mehrere Sichten auf ein Modell möglich sind und Änderungen konsistent von allen Sichten (inklusive des "`Propertysheets"') erfasst werden können, wird automatisch eine Adpater-Schicht generiert, die eine command-ähnliche Funktion inne hat. Für alle Setter wird bereits automatisch ein Event-Handling generiert, so dass Änderungen an Eigenschaften der Modell-Entitäten von beliebigen Betrachtern beobachtet werden können. Um auch eine Unterstützung für das Eclipse-Framework außerhalb des Modell-Editor-Plugins bieten zu können, werden die intern als \textit{EMF adapter notification} bezeichneten Aufrufe durch die Adapterschicht in \texttt{fireNotifyChanged} Ereignisse des Eclipse-Frameworks übersetzt.
		
		Die Erzeugung von Adaptoren ist wiederum über Factories gekapselt.
		\item \textbf{Reflective API manipulation} EMF bietet eine Reflective API, um Instanzen eines Modells zu erzeugen (vgl. \cite{devx}), zu initialisieren und zuzugreifen oder dynamisch Modelle zu erstellen. Analog zur Java-Methode \texttt{Object.} \texttt{getClass()} bietet \texttt{eClass()} die Möglichkeit Meta-Daten einer Instanz (etwa die \texttt{EClass}) in Erfahrung zu bringen. Auf jedem Objekt kann der Zugriff auf Daten per \texttt{eGet()} und \texttt{eSet(}) erfolgen, ähnlich dem in Java üblichen Zugriff über \texttt{java.lang.reflect.Method.invoke()}. Zusätzlich bietet jedes Instanz"=Objekt Zugriff auf seine Container (\textit{Parent}-Beziehung) über \texttt{eContainer()}. Da \texttt{EObject} von \texttt{Notifier} erbt, ist es möglich alle Änderungen von Objekt-Daten zu überwachen (\textit{monitoring}).
		\item \textbf{Validation"=Framework} EMF unterstützt die Validierung von Modell"=Constraints. Constraints können zum Beispiel über XML"=Schemata ausgedrückt und mit einem Bezeichner versehen werden. Im annotierten Java erfolgt dann eine Referenzierung der Constraints:
\begin{verbatim}
      @model
        annotation="[URI - http://www.here.com]
        constraints='[constraint name]'"
\end{verbatim}
im Bereich der JavaDocs.

Zudem ist die Definition von Constraints auch im generierten Java"=Quellcode möglich. Siehe hierzu auch \cite{emf-validation}.
		\item \textbf{Queries} Queries ermöglichen das gezielte Durchsuchen von EMF-Modellen, um an Entitäten des Modells zu gelangen oder Struktur-Informationen zu extrahieren.
	\end{itemize}
	
	\item\label{enum:RoundtripZyklus} Der bereits oben angedeutete Roundtrip-Zyklus (in der Abbildung \ref{fig:prozess01} als Schleife oben rechts erkennbar) schließt mit der Repräsentation des Modells in anotiertem Java, das wiederum von EMF importiert werden kann. Damit ist eine Synchronisation zwischen EMF-Modell und der Modell-Repräsentation im Java-Quellcode möglich.
	
	Dennoch scheint dieser Roundtrip-Zyklus für die Diplomarbeit nicht angeraten, da die unter Schritt \ref{enum:ECORE} angeführte Modell-Repräsentation im ECORE-Modell nicht konsistent mitgepflegt würde. Um auch diesen Schritt des Modellierungsprozesses konsistent zu halten, sollten Modelliterationen nicht über den Roundtrip-Zyklus vorgenommen werden, sondern stets mit Modifikationen des ECORE-Modells beginnen. Unter der Annahme, dass der generierte Quellcode nach einer Modelländerungen problemlos mit manuellen Änderungen zusammengeführt (\textit{merge}) werden kann, läßt sich der skizzierte Prozess ohne den Re-Import von anotiertem Java stets wiederholen, wobei das Modell schrittweise verfeinert werden kann.
	
	Zusammengefasst erscheint eine Nutzung der Roundtrip-Möglichkeit mit dem Import von anotiertem Java zunächst nicht sinnvoll, da dass ECORE-Modell nicht in den Roundtrip-Zyklus einbezogen werden könnte.
	
	\item Der nächste Schritt des Prozesses ist die Transformation unter Verwendung von GMF bzw. Merlin. Wie bereits in Kapitel \ref{sec:Merlin-GMF} angesprochen, zielen Merlin und GMF zu Teilen in die gleiche Richtung: "`Eine generative Brücke zwischen EMF und GEF"'.
	
	Während Merlin in erster Linie Code-Generierung und Model-Transformationen unterstützt, zielt GMF direkt auf die Erzeugung von GEF-Editoren. Im Merlin-Kontext nimmt die GEF-Generierung derzeit eine Nebenrolle ein. Hier sind vordefinierte Templates verfügbar, die einen einfachen graphischen Editor erzeugen.
	
	Wie bereits oben angesprochen, gibt es Bestrebungen, Merlin in das GMF-Projekt zu überführen. Da GMF zum derzeitigen Zeitpunkt bereits mächtiger als Merlin erscheint und laufend an Stabilität gewinnt, ist mit einer sinnvoll verwendbaren Version für die Zeit der geplanten Transformationen zu rechnen. Vor diesem Hintergrund bietet sich die Verwendung von GMF für die Transformationsprozesse in der Diplomarbeit an.
	
	Wie in Abbildung \ref{fig:prozess01} angedeutet, sind zur gezielten Erzeugung eines GEF"=Editors zusätzliche Transformationsanweisungen (\textit{Transformations}) notwendig. In diesen Transformationsanweisungen muss unter anderem hinterlegt werden, ob und wie Assoziationsklassen  dargestellt werden sollen, welche Darstellungssymbole verwendet werden sollen und welche Elemente in der Werkzeugleiste erscheinen sollen. Die Transformationsanweisungen fließen schließlich in den generierten GEF-Editor ein.
	
	\item Der GEF-Editor stellt das nahezu fertige Produkt dar. Realistischer Weise ist jedoch davon auszugehen, dass trotz stark anpassbarer Transformationen kleinere Änderungen manuell nachzupflegen bleiben. Idealerweise ist der hier notwendige Aufwand möglichst klein. Da diese manuellen Modifikationen nach jeder Änderungen des ECORE-Modells durchzuführen wären, entscheidet sich an dieser Stelle, ob der generative Ansatz leistungsfähiger als der der vollständig manuellen Implementierung ist.
	
	\item Der Editor ist das Endprodukt, dass jederzeit nach Änderungen am zu Grunde liegenden ECORE-Modell durch einen neuerlichen Durchlauf des Generierungsprozesses entstehen soll. Dieses Produkt muss sich unter anderem mit dem Ergebnis der Projektgruppe Ride.NET \cite{RideDotNet} messen lassen, deren Ziel die Implementierung eines Editors des Palladio Komponentenmodells unter C\# war.
\end{enumerate}



\subsubsection{Modellierungsprozess}
\label{sec:Modellierungsprozess}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.00\textwidth]{./image/model-process01.pdf}
	\caption{Modellierungs--Prozess}
	\label{fig:model-process01}
\end{figure}

Im Kern soll sich die Diplomarbeit mit der Meta-Modellierung des Palladio Komponentenmodells beschäftigen. In Abbildung \ref{fig:model-process01} wird eine iterative Variante zur Entwicklung des Meta-Modells dargestellt. Dazu wird der in Abbildung \ref{fig:prozess01} dargestellte obere Bereich hervorgehoben. Im Gegensatz zur Darstellung des Gesamtprozesses wird jedoch bewusst der Roundtrip-Zyklus über den Import von anotiertem Java ausgeblendet. Wie bereits im vorigen Kapitel erläutert wurde, würde bei diesem Vorgehen die ECORE-Modell-Darstellung "`out-of-sync"' geraten.

Statt dessen ist eine Evaluation der generierten Modelle mittels kleinerer Modellierungstests vorgesehen. Durch mehrere Iterationen soll das Meta-Modell des Palladio Komponentenmodells verfeinert werden. Dieses Verfahren ist vor allem notwendig, weil es bisher keine abgeschlossene vollständige formale Darstellung des Palladio Komponentenmodells gibt. Daher muss der skizzierte Modellierungs-Prozess zugleich als ein Verfahren zur Ermittlung von Anforderungen aufgefasst werden. In diesem Verfahren geht es darum, zu prüfen, ob sich alle (theoretischen) Konzepte, die dem Palladio Komponentenmodell zu Grunde liegen, ebenfalls im EMF Meta"=Modell widerspiegeln, bzw. mit dem Meta"=Modell abbilden lassen.

Der iterative Prozess sieht daher wie folgt aus:
\begin{enumerate}
	\item Das ECORE-Modell wird initial modelliert.
	\item Aus dem ECORE-Modell wird mit Hilfe des EMF-Generators eine Abbildung auf Java-Code vorgenommen.
	\item In der Evaluationsphase wird geprüft, ob die Umsetzung des Modells
	\begin{itemize}
		\item vollständig
		\item korrekt
		\item applizierbar
	\end{itemize}
	ist. Damit wird sichergestellt, ob sich alle Konzepte des Palladio Komponentenmodells umsetzen lassen. Oder es wird festgestellt, welche Konzepte sich nicht über ein ECORE-Modell darstellen lassen und welche Einschränkungen hierfür der Grund sind. Zudem wird darauf geachtet, dass das richtige Meta-Modell entworfen wird und das Meta-Modell richtig entworfen wird. Nicht zuletzt führt der praktische Test zu einem anwendbaren Modell, das sich für Test-Modelle bereits bewährt hat.
	
	Der iterative Ansatz ist der Wartbarkeit als zuträglich aufzufassen. Da das Palladio Komponentenmodell in der Vergangenheit diverse Änderungen und Erweiterungen erfahren hat, ist davon auszugehen, dass auch zukünftig weitere Modifikationen zu erwarten sind. Durch die Wahl mehrerer Iterationsschritte wird die Wartbarkeit / Änderungsfreundlichkeit des zu entwickelnden Meta-Modells unmittelbar getestet.
\end{enumerate}

Zum jetzigen Zeitpunkt ist bereits absehbar, dass die Erweiterbarkeit des Meta-Modells um variable, zuvor nicht typisierte Zusatzattribute  Teil eines fest geplanten Iterationsschrittes sein wird. Siehe hierzu auch Kapitel \ref{sec:Fragestellungen}.

Das in der Abbildung \ref{fig:model-process01} angeführte "`Feedback"' wird von den Mitgliedern der Forschungsgruppe Palladio und vom Diplomanden durch eine kritische Reflexion vorgenommen. Eine klare konzeptionelle Abgrenzung des Komponentenmodells ist dabei ebenfalls Teil der Diplomarbeit.



\subsection{Fragestellungen}
\label{sec:Fragestellungen}
Der Diplomarbeit liegen unter anderem die folgenden Fragestellungen zu Grunde:
\begin{itemize}
	\item Widersprechen sich Modellierungskonzepte des Palladio Komponentenmodells? Sind Einschränkungen des Komponentenmodells erkennbar? Wurden auf Grund fehlender formaler Manifestierung des Modells Unvollständigkeiten übersehen? Welche Semantik verbirgt sich hinter spezifischen Modellierungskonstrukten?
	\item Das Palladio Komponentenmodell ist zu kleinen Teilen nur implizit in den Köpfen der Mitglieder der Palladio-Gruppe vorhanden. Gibt es Widersprüche in den impliziten Annahmen?
	\item Welche der konzeptionellen Aspekte des Palladio Komponentenmodells lassen sich mit den gewählten Hilfsmitteln wie UML, ECORE, EMF und anotiertem Java abbilden?
	\item Welche Aspekte lassen sich nicht abbilden und aus welchen Gründen scheitert diese Abbildung. Welche Einschränkungen gelten für UML, ECORE, EMF und anotiertem Java? Welche Erweiterungen der (Abbildungs-) Modelle müssten vorgenommen werden.
	\item Welche Wege bieten sich zur Modellierung an? Welche Alternativen gibt es bei der Modellierung? Aus welchen Gründen wurden welche Alternativen gewählt? Gibt es gleichwertige Alternativen oder widersprüchliche Herangehensweisen?
	\item Wie lassen sich variable Zusatzattribute im Meta-Modell unterbringen und wie finden sich diese Zusatzattribute im generierten Modell-Code bzw. einem Modell-Editor wieder?
	\item Welche technologischen und konzeptionellen Einschränkungen bringen die verwendeten Generatoren und Transformatoren mit sich?
\end{itemize}



\subsection{Annahmen}
\label{sec:Annahmen}
Um die Diplomarbeit entsprechend der Planungen aus Kapitel \ref{sec:GeplanterProzess} durchführen zu können, müssen einige Annahmen erfüllt sein. Diese Annahmen lassen sich nicht in vollem Umfang überprüfen, da dies wesentliche Teile der Diplomarbeit vorweg nehmen würde.

\begin{itemize}
	\item UML, ECORE, EMF und anotiertes Java müssen mächtig genug sein, um darin alle Konzepte des Palladio Komponentenmodells abzubilden.
	\item Um die bereits angesprochenen variablen Zusatzattribute zu Modell-Entitäten zu ermöglichen, ist es notwendig, dass dies durch die Meta-Modellierung auf den folgenden Ebenen unterstützt wird: UML, ECORE, EMF und anotiertem Java. Für UML bedeutet dies, dass das Meta-Modell für ein gegebenes Zusatzattribut dynamisch vom Kern-Meta-Modell (Meta-Modell ohne Zusatzattribute) referenziert werden kann.
	\item Ferner wird angenommen, dass der zu verwendende Generator von EMF ausreichend mächtig ist, um alle Aspekte des Palladio Komponentenmodells in anotiertem Java abzubilden.
	\item Die Möglichkeiten die Transformationen von GMF eigenen Bedürfnissen anzupassen müssen ausreichend groß sein, um einen den Benutzerbedürfnissen entsprechenden Editor generieren zu können.
	\item Wie bereits angesprochen, muss davon ausgegangen werden, dass der in Kapitel \ref{sec:Gesamtprozess} skizzierte Prozess in mehreren Iterationen (insbesondere auf Grund der "`konzeptionellen Variabilität"' des Palladio Komponentenmodells) durchlaufen werden muss. Da nicht davon ausgegangen werden kann, dass jegliche generierten Modelle und Code ohne manuelle Nachpflege verwendet werden können, ergibt sich zwingend die Anforderung, dass der EMF-Generator und auch GMF funktionierende "`Merge"'-Mechanismen implementieren, die verschiedene Modellversionen (zumindest nach kleineren Änderungen) unter Erhaltung manueller Änderungen zusammenführen können. Nur mit einem solchen Mechanismus lässt sich der iterative Prozess sinnvoll durchführen.
	
	Zum Zeitpunkt der Niederschrift dieses Proposals wird ein solcher Merge"=Mechanismus von Merlin nicht unterstützt.
\end{itemize}

Sind nicht alle hier dargestellten Annahmen erfüllt, hat dies direkte Auswirkungen auf die Umsetzung der Diplomarbeit. Daher sind die hier genannten Annahmen auch im Zusammenhang mit den Risiken zur Umsetzung der Diplomarbeit (siehe Kapitel \ref{sec:Risikomanagement}) zu sehen.



%\subsection{Fehlerquellen}
%\label{sec:Fehlerquellen}



\subsection{Einschränkungen}
\label{sec:Einschraenkungen}
Im Rahmen der Diplomarbeit ist \textit{nicht} geplant, einen vollständigen, industriellen Standards genügenden, GUI-Editor für das Palladio Komponentenmodell zu erzeugen. Im Vordergrund steht die Modellierung der EMF-Darstellung des dem Editor als Basis dienenden Komponenten-Meta-Modells und die Erprobung möglicher Wege zur Generierung eines vollständigen Editors.



\subsection{Risikomanagement}
\label{sec:Risikomanagement}
Die Durchführung der Diplomarbeit unterliegt vielfältigen Risiken. Ändern sich Grundannahmen, wie in Kapitel \ref{sec:Annahmen} dargestellt, kann ein erfolgreicher Abschluss der Diplomarbeit gefährdet sein. Um Risiken zu minimieren werden an dieser Stelle für wahrscheinliche Risiken Auswege aufgezeigt.

\paragraph{Konzeptioneller Wandel im Palladio Komponentenmodell} Wie bereits mehrfach in vorangegangenen Kapiteln erwähnt wurde, ist das Palladio Komponentenmodell aktiver Gegenstand von Forschungsarbeit. Damit verbunden ist das Risiko, dass sich Modell"=Konzepte ändern können, Modell"=Erweiterungen ergänzt werden und nicht mehr benötigte oder problematische Teile des Modells wegfallen können. Um das Risiko in diesem Bereich einzugrenzen, bieten sich zwei Strategien an:
\begin{enumerate}
	\item Wird die Version des Komponentenmodells für die Diplomarbeit eingefroren, kommen konzeptionelle Änderungen in der Diplomarbeit nicht zum Tragen. Dieser Ansatz birgt jedoch die Gefahr, dass die Modellversionen in der Diplomarbeit und der realen Entwicklung stark divergieren.
	\item Änderungen am Komponentenmodell können genau beobachtet werden und nur wenn Änderungen als unkritisch eingestuft werden, fließen sie in die Diplomarbeit ein. Dies hat den Vorteil, dass das Meta-Modell aus der Diplomarbeit stärker dem realen Stand entspricht.
\end{enumerate} 

\paragraph{Annahmen nicht erfüllt} Sind Annahmen, die in Kapitel \ref{sec:Annahmen} festgestellt wurden, nicht erfüllt, sind hiermit die größten Projektrisiken verbunden. Je nachdem, zu welchem Zeitpunkt im Prozess die nicht erfüllten Annahmen zum Tragen kommen, ist anzunehmen, dass die Erfüllung aller späteren Teile des Prozesses nur noch unwahrscheinlich ist. Das bedeutet, dass Abstriche im Umfang der Diplomarbeit zu erwarten sind. Um diesen Ausfall abzumildern, könnten beispielsweise für die verwendeten Modelle und Werkzeuge Alternativ-Produkte verwendet werden.

\paragraph{Verfügbarkeit verwendeter Werkzeuge} Sollten im Rahmen der Diplomarbeit verwendete Werkzeuge nicht mehr verfügbar sein (etwa aus Gründen von Lizenzproblemen), sollte versucht werden, geeignete Alternativen zu finden und zu verwenden. Insbesondere im Falle von GMF baut die Diplomarbeit darauf, dass dieses Werkzeug, wie in der Vergangenheit, weiterentwickelt wird. Da hinter GMF maßgeblich IBM und Borland mit professionellen, bezahlten Entwicklern stehen, ist es notwendig, dass diese Firmen weiterhin Interesse an der Entwicklung haben und keinen strategischen Wechsel vollziehen. Zudem ist die Diplomarbeit davon abhängig, dass das Projekt ausreichend schnell voranschreitet und in einen stabilen Zustand gelangt.

Sollten Probleme mit GMF auftreten, bietet sich ein Rückgriff auf Merlin oder die bis dahin letzte stabile Entwicklerversion an.

\paragraph{Vertikaler Prototyp}
Zur weiteren prophylaktischen Minimierung von Risiken wurde bereits erfolgreich (wenn auch mit gewissen Einschränkungen) ein vertikaler Prototyp unter Verwendung von Merlin erzeugt. Als einer der ersten Schritte der Diplomarbeit ist ein weiterer vertikaler Prototyp geplant, der später auch Basis darauf folgender Iterationen sein wird. Dieser soll GMF verwenden und variable Zusatzattribute testen.



\section{Durchführung}
\label{sec:Durchfuehrung}
\subsection{Betreuer}
\label{sec:Betreuer}
\begin{itemize}
	\item \textbf{Erstgutachter:} Jun.-Prof. Dr. Ralf Reussner
	\item \textbf{Zweitgutachter:} Prof. Dr. Wilhelm Hasselbring
	\item \textbf{Betreuer:} Dipl.-Wirtsch.-Inform. Steffen Becker
\end{itemize}



\subsection{Artefakte}
\label{sec:Artefakte}
Für den Verlauf der Diplomarbeit sind die folgenden Artefakte vorgesehen:
\begin{itemize}
	\item Proposal (dieses Dokument), $\ast$
	\item Begleitfolien zum Proposal, $\ast$
	\item Quellcode anläßlich der Iterationen laut Kapitel \ref{sec:Iterationsschritte}
	\item Diplomarbeit, $\ast$
	\item Begleitfolien zur Diplomarbeit, $\ast$
	\item Quelltexte und Modell-Serialisierungen der finalen Version des Meta-Modells, $\ast$
\end{itemize}
$\ast$ kennzeichnet für die Veröffentlichung bestimmte Artefakte.


\subsection{Entwicklungsumgebung und Werkzeuge}
\label{sec:EntwicklungsumgebungundTools}
Als Entwicklungsumgebung wird Eclipse verwendet. Zusätzlich zu verwendende Werkzeuge sind: Rational Software Architect sowie als Ergänzung des Eclipse-Frameworks die Eclipse-Plugins EMF, GMF, Merlin, UML2, GEF und Omondo UML.

Daneben sind vorgesehen: LaTeX, TeXnicCenter, CVS und Visio 2003.



\subsection{Vorgehensmodell}
\label{sec:Vorgehensmodell}
Wie bereits im Kapitel \ref{sec:GeplanterProzess} genannt, wird für die Diplomarbeit ein iteratives und inkrementelles Vorgehen gewählt. Dies hat ebenfalls einen positiven Effekt auf die Projektrisiken, da bereits zu einem frühen Zeitpunkt ein lauffähiges Produkt zur Verfügung steht, das ein gänzliches Scheitern der Diplomarbeit verhindert.



\subsection{Zeitplanung}
\label{sec:Zeitplanung}
Entsprechend der Vorgaben der DPO4 vom 01.02.2002 beträgt die Dauer der Diplomarbeit sechs Monate. Die Diplomarbeit beginnt am 01.01.2006 und endet entsprechend spätestens am 31.06.2006. Während der Diplomarbeit finden in der Regel wöchentliche Besprechungen mit den Betreuern statt.



\subsubsection{Gantt-Charts}
\label{sec:Gantt-Charts}
\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.60\textwidth]{./image/zeitplanung01.pdf}
	\caption{Zeitplanung der Diplomarbeit}
	\label{fig:zeitplanung01}
\end{figure}
In Abbildung \ref{fig:zeitplanung01} wird der geplante zeitliche Verlauf der Diplomarbeit, inklusive aller wichtigen Meilensteine, dargestellt.

Die Termine für die Vorträge im DuD-Seminar stehen derzeit noch nicht fest, sind in der Gantt-Chart-Darstellung jedoch bereits enthalten.



\subsubsection{Iterationsschritte}
\label{sec:Iterationsschritte}
Für die Diplomarbeit werden die folgenden Iterationsschritte angestrebt, deren zeitliche Terminierung Kapitel \ref{sec:Gantt-Charts} zu entnehmen ist:
\begin{itemize}
	\item \textbf{Vertikaler Prototyp unter Verwendung von GMF und dynamischer Annotation.} Dieser Iterationsschritt umfasst eine prototypische Umsetzung ausgewählter Modellkonstrukte des Komponentenmodells, etwa das Anlegen von Komponenten und Schnittstellen mit der Möglichkeit zur Verfeinerung auf einer weiteren Modellierungsebene des Komponentenmodells, in EMF.
	
	Zusätzlich sollen dynamisch Zusatzattribute zu Entitäten anotiert werden können. Hier geht es wiederum primär um die Prüfung der Machbarkeit. Aufgezeigt werden sollen mögliche Wege zur Umsetzung der dynamischen Anotationen in EMF.
	
	Die Verwendung von GMF soll schließlich sicherstellen, dass sich die verwendeten Konzepte aus EMF problemlos in den Transformationen von GMF verwenden lassen, um zu einem lauffähigen GUI-Editor gelangen zu können.
	
	\item \textbf{1. Iteration der Modellierung in EMF.} Als Erweiterung des Prototypen aus dem vorangegangenen Iterationsschritt, werden in diesem Schritte zusätzliche Modellkonstrukte des Komponentenmodells in das EMF-Modell übernommen. Insbesondere sollen in diesem Schritt alle Entitäten des Komponentenmodells in EMF realisiert werden. Dabei ist zunächst an die Modellierung im Bereich der Typ-Ebene gedacht.
	
	\item \textbf{2. Iteration der Modellierung in EMF.} Erweiterungen dieser Iteration sollen ebenfalls alle vom Komponentenmodell definierten Ebenen sowie die zwischen den Ebenen definierten \textit{Constraints} berücksichtigen. Die \textit{Constraints} sollen dabei, soweit möglich, programmatisch erfasst werden, um damit automatisiert prüfbar zu sein.
	
	\item \textbf{GMF-Transformationen definieren, 1. Iteration; Editor-Funktionalität für das gesamte EMF-Modell herstellen.} In diesem Schritt werden GMF-Transformormationen für das dann vorliegende EMF"=Modell definiert. Das Augenmerk liegt dabei vor allem beim \textit{Proof"=of"=Concept}. Es geht also vor allem darum, eventuelle Modellierungsfehler abzufangen, die einer Umsetzung im einem GUI"=Editor zuwider laufen. Daher kann in diesem Iterationsschritt zusätzlich eine Modifikation der EMF-Modellierung aus den vorangegangenen Schritten anfallen.
	
	\item \textbf{GMF-Transformationen definieren, 2. Iteration; Anpassung des generierten Editors (\textit{Customizing}).} Dieser Schritt befasst sich mit der näheren Ausgestaltung von GMF"=Transformationen um damit einen, an die Bedürfnisse der Palladio"=Gruppe angepassten, GUI-Editor zu erzeugen. Insbesondere Fragen der \textit{Usability} und der Interpretation der Darstellung von Modellierungsebenen sollen in diesem Schritt überprüft werden.
	
	\item \textbf{Manuelle Anpassung; eventuell vorhandene Fehler korrigieren.} Zuletzt wird in diesem Iterationsschritt vorgesehen, eventuelle manuelle Optimierungen eines generierten GUI"=Editor"=Prototypen durchzuführen. Zudem soll ergründet werden, wie umfangreich eventuelle manuelle Modifikationen sind, damit ein generierter GUI"=Editor den Anforderungen der Palladio"=Gruppe genügt.
\end{itemize}






%____________________________________________________________________________________________________________
%____________________________________________________________________________________________________________
%____________________________________________________________________________________________________________
%
% "`Hineinlinken"' von Zusatzinformationen in UML muss möglich sein. Zunächst Linking und Referenzieren ausprobieren.
%
% Wichtig: Trennung von Shapes und Entitäten
%
% KM3 Eclipse --> Ascii-Repräsentation

